{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b171458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698f7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1. INPUT: Path to your JSON file containing image paths and their corresponding prompts.\n",
    "JSON_INPUT_FILE = \"clip.json\" \n",
    "\n",
    "# 2. OUTPUT: The main directory where all results will be saved.\n",
    "#    Subfolders for each experiment will be created automatically inside this directory.\n",
    "MAIN_OUTPUT_DIR = \"./Output/CLIP_Ranking_Results\"\n",
    "\n",
    "# 3. MODEL: The OpenCLIP model to use for evaluation.\n",
    "MODEL_NAME = 'ViT-B-32'\n",
    "PRETRAINED_DATASET = 'laion2b_s34b_b79k'\n",
    "\n",
    "# 4. THRESHOLD: How much above the average score an image must be to be considered \"good\".\n",
    "#    For example, 1.05 means 5% above the average.\n",
    "THRESHOLD_MULTIPLIER = 1.05 \n",
    "\n",
    "# 5. NEGATIVE PROMPT: A general negative prompt to apply to all experiments.\n",
    "NEGATIVE_PROMPT = \"bad anatomy, bad hands, blurry, low resolution, worst quality, jpeg artifacts, ugly, deformed, disfigured, text, watermark, signature, cartoon, 3d, cgi, anime\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19464ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    \"\"\"Loads the OpenCLIP model, preprocessor, and tokenizer.\"\"\"\n",
    "    print(f\"Loading model: {MODEL_NAME} pretrained on {PRETRAINED_DATASET}...\")\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED_DATASET)\n",
    "    tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device).eval()\n",
    "    print(f\"Model loaded successfully on device: {device}\")\n",
    "    return model, preprocess, tokenizer, device\n",
    "\n",
    "def group_data_by_experiment(json_path):\n",
    "    \"\"\"Parses the JSON file and groups image paths and prompts by experiment name.\"\"\"\n",
    "    print(f\"Grouping data from {json_path}...\")\n",
    "    try:\n",
    "        df = pd.read_json(json_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "    experiments = {}\n",
    "    for _, row in df.iterrows():\n",
    "        path = row['image_path']\n",
    "        prompt = row['prompt']\n",
    "        try:\n",
    "            exp_name = path.split('/')[1]\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Could not determine experiment name for path '{path}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        if exp_name not in experiments:\n",
    "            experiments[exp_name] = {'prompt': prompt, 'images': []}\n",
    "        experiments[exp_name]['images'].append(path)\n",
    "        \n",
    "    print(f\"Found {len(experiments)} experiments to process.\")\n",
    "    return experiments\n",
    "\n",
    "def process_experiment(exp_name, data, model, preprocess, tokenizer, device, neg_embed, output_dir_base):\n",
    "    \"\"\"Processes a single experiment: scores images, ranks them, and saves all outputs.\"\"\"\n",
    "    print(f\"\\n--- Starting Experiment: {exp_name} ---\")\n",
    "    \n",
    "    positive_prompt = data['prompt']\n",
    "    image_paths = data['images']\n",
    "    \n",
    "    exp_output_dir = os.path.join(output_dir_base, exp_name)\n",
    "    good_images_dir = os.path.join(exp_output_dir, \"good_images\")\n",
    "    os.makedirs(good_images_dir, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pos_tokens = tokenizer([positive_prompt]).to(device)\n",
    "        pos_embed = model.encode_text(pos_tokens).float()\n",
    "        pos_embed /= pos_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    results = []\n",
    "    for path in tqdm(image_paths, desc=f\"Scoring images for {exp_name}\"):\n",
    "        try:\n",
    "            image = preprocess(Image.open(path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                img_embed = model.encode_image(image).float()\n",
    "                img_embed /= img_embed.norm(dim=-1, keepdim=True)\n",
    "                pos_score = (img_embed @ pos_embed.T).item()\n",
    "                neg_score = (img_embed @ neg_embed.T).item()\n",
    "                final_score = pos_score - neg_score\n",
    "                results.append((os.path.basename(path), final_score, pos_score, neg_score))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found, skipping: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(path)}: {e}\")\n",
    "\n",
    "    if not results:\n",
    "        print(f\"No images were successfully processed for experiment {exp_name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"Image\", \"Score\", \"Positive\", \"Negative\"])\n",
    "    df.sort_values(by=\"Score\", ascending=False, inplace=True)\n",
    "    df.insert(0, \"Rank\", range(1, len(df) + 1))\n",
    "    \n",
    "    average_score = df[\"Score\"].mean()\n",
    "    good_threshold = average_score * THRESHOLD_MULTIPLIER\n",
    "    print(f\"Average Score: {average_score:.4f} | Dynamic Good Threshold: {good_threshold:.4f}\")\n",
    "\n",
    "    csv_path = os.path.join(exp_output_dir, f\"ranked_results_{exp_name}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Ranked CSV saved to: {csv_path}\")\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(df[\"Rank\"], df[\"Score\"], marker='o', linestyle='-', color='b', label='Total Score (Pos - Neg)')\n",
    "    plt.axhline(y=good_threshold, color='g', linestyle='--', label=f'Good Threshold ({good_threshold:.2f})')\n",
    "    plt.axhline(y=average_score, color='r', linestyle=':', label=f'Average Score ({average_score:.2f})')\n",
    "    plt.title(f\"CLIP Score Ranking for: {exp_name}\", fontsize=16)\n",
    "    plt.xlabel(\"Rank\", fontsize=12)\n",
    "    plt.ylabel(\"Score\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(exp_output_dir, f\"rank_plot_{exp_name}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {plot_path}\")\n",
    "\n",
    "    good_images_df = df[df[\"Score\"] >= good_threshold]\n",
    "    print(f\"Found {len(good_images_df)} images meeting the 'good' threshold.\")\n",
    "    for _, row in good_images_df.iterrows():\n",
    "        original_path = next((p for p in image_paths if os.path.basename(p) == row[\"Image\"]), None)\n",
    "        if original_path:\n",
    "            try:\n",
    "                dest_path = os.path.join(good_images_dir, row[\"Image\"])\n",
    "                Image.open(original_path).save(dest_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to copy {row['Image']}: {e}\")\n",
    "    print(f\"Good images saved to: {good_images_dir}\")\n",
    "    \n",
    "print(\"✅ Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d639bfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: ViT-B-32 pretrained on laion2b_s34b_b79k...\n",
      "Model loaded successfully on device: cpu\n",
      "Grouping data from clip.json...\n",
      "Found 5 experiments to process.\n",
      "✅ Initial setup complete. Negative prompt is encoded.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model, preprocess, tokenizer, device = load_model()\n",
    "\n",
    "# Group the input images and prompts from the JSON file\n",
    "experiments = group_data_by_experiment(JSON_INPUT_FILE)\n",
    "\n",
    "# Pre-encode the global negative prompt once for efficiency\n",
    "if experiments:\n",
    "    with torch.no_grad():\n",
    "        neg_tokens = tokenizer([NEGATIVE_PROMPT]).to(device)\n",
    "        neg_embed = model.encode_text(neg_tokens).float()\n",
    "        neg_embed /= neg_embed.norm(dim=-1, keepdim=True)\n",
    "    print(\"✅ Initial setup complete. Negative prompt is encoded.\")\n",
    "else:\n",
    "    print(\"❌ No experiments found or error in loading data. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dba1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Experiment: exp1_sd15_2050 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring images for exp1_sd15_2050: 100%|██████████| 60/60 [00:05<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.0669 | Dynamic Good Threshold: 0.0703\n",
      "Ranked CSV saved to: ./Output/CLIP_Ranking_Results\\exp1_sd15_2050\\ranked_results_exp1_sd15_2050.csv\n",
      "Plot saved to: ./Output/CLIP_Ranking_Results\\exp1_sd15_2050\\rank_plot_exp1_sd15_2050.png\n",
      "Found 25 images meeting the 'good' threshold.\n",
      "Good images saved to: ./Output/CLIP_Ranking_Results\\exp1_sd15_2050\\good_images\n",
      "\n",
      "--- Starting Experiment: exp2_sd15_3050 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring images for exp2_sd15_3050: 100%|██████████| 60/60 [00:05<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.0898 | Dynamic Good Threshold: 0.0943\n",
      "Ranked CSV saved to: ./Output/CLIP_Ranking_Results\\exp2_sd15_3050\\ranked_results_exp2_sd15_3050.csv\n",
      "Plot saved to: ./Output/CLIP_Ranking_Results\\exp2_sd15_3050\\rank_plot_exp2_sd15_3050.png\n",
      "Found 30 images meeting the 'good' threshold.\n",
      "Good images saved to: ./Output/CLIP_Ranking_Results\\exp2_sd15_3050\\good_images\n",
      "\n",
      "--- Starting Experiment: exp3_sd15_3050_v2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring images for exp3_sd15_3050_v2: 100%|██████████| 60/60 [00:07<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.1099 | Dynamic Good Threshold: 0.1154\n",
      "Ranked CSV saved to: ./Output/CLIP_Ranking_Results\\exp3_sd15_3050_v2\\ranked_results_exp3_sd15_3050_v2.csv\n",
      "Plot saved to: ./Output/CLIP_Ranking_Results\\exp3_sd15_3050_v2\\rank_plot_exp3_sd15_3050_v2.png\n",
      "Found 25 images meeting the 'good' threshold.\n",
      "Good images saved to: ./Output/CLIP_Ranking_Results\\exp3_sd15_3050_v2\\good_images\n",
      "\n",
      "--- Starting Experiment: exp4_sd15_3050_v3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring images for exp4_sd15_3050_v3: 100%|██████████| 60/60 [00:05<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.0875 | Dynamic Good Threshold: 0.0919\n",
      "Ranked CSV saved to: ./Output/CLIP_Ranking_Results\\exp4_sd15_3050_v3\\ranked_results_exp4_sd15_3050_v3.csv\n",
      "Plot saved to: ./Output/CLIP_Ranking_Results\\exp4_sd15_3050_v3\\rank_plot_exp4_sd15_3050_v3.png\n",
      "Found 29 images meeting the 'good' threshold.\n",
      "Good images saved to: ./Output/CLIP_Ranking_Results\\exp4_sd15_3050_v3\\good_images\n",
      "\n",
      "--- Starting Experiment: exp5_sdxl_a6000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring images for exp5_sdxl_a6000: 100%|██████████| 60/60 [00:07<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.0978 | Dynamic Good Threshold: 0.1027\n",
      "Ranked CSV saved to: ./Output/CLIP_Ranking_Results\\exp5_sdxl_a6000\\ranked_results_exp5_sdxl_a6000.csv\n",
      "Plot saved to: ./Output/CLIP_Ranking_Results\\exp5_sdxl_a6000\\rank_plot_exp5_sdxl_a6000.png\n",
      "Found 23 images meeting the 'good' threshold.\n",
      "Good images saved to: ./Output/CLIP_Ranking_Results\\exp5_sdxl_a6000\\good_images\n",
      "\n",
      "✅ All experiments processed successfully!\n"
     ]
    }
   ],
   "source": [
    "if experiments:\n",
    "    # Process each experiment sequentially\n",
    "    for exp_name, data in experiments.items():\n",
    "        process_experiment(exp_name, data, model, preprocess, tokenizer, device, neg_embed, MAIN_OUTPUT_DIR)\n",
    "        \n",
    "    print(\"\\n✅ All experiments processed successfully!\")\n",
    "else:\n",
    "    print(\"❌ Cannot run experiments because data was not loaded correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
