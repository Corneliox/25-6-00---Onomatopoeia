{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148c32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch pillow pandas matplotlib\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedfaa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6916451371480ab6428ac5238202be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4bfdfa4eb046369459492cd26a611a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pongo\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Pongo\\.cache\\huggingface\\hub\\models--Salesforce--blip-vqa-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f69d0b3c654f81ab2499bbeb819127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8296c3ba234a8e8ea68bb97c9eb9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be262e7d97546a593598c27eafe1220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028a4497da884074a4128af4b7e5d82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5e6e3497504f32bc73de7d06756797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b2540d84a64328b63ef6a4288e78a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model BLIP VQA loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# BLIP VQA model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
    "\n",
    "print(\"✅ Model BLIP VQA loaded on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e942aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daftar pertanyaan VQA: ['Does the image contain onomatopoeia text?', 'What text is written in the image?', 'What are the dominant colors of the water splash?']\n"
     ]
    }
   ],
   "source": [
    "with open(\"vqa_question.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    q_data = json.load(f)\n",
    "\n",
    "questions = q_data[0][\"questions\"]\n",
    "print(\"Daftar pertanyaan VQA:\", questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"dataset_eval/\"\n",
    "results = []\n",
    "\n",
    "for exp in os.listdir(base_dir):\n",
    "    exp_path = os.path.join(base_dir, exp)\n",
    "    if not os.path.isdir(exp_path):\n",
    "        continue\n",
    "    \n",
    "    for img_file in os.listdir(exp_path):\n",
    "        if not img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(exp_path, img_file)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        for q in questions:\n",
    "            inputs = processor(image, q, return_tensors=\"pt\").to(device)\n",
    "            out = model.generate(**inputs)\n",
    "            answer = processor.decode(out[0], skip_special_tokens=True)\n",
    "            \n",
    "            results.append({\n",
    "                \"experiment\": exp,\n",
    "                \"image\": img_file,\n",
    "                \"question\": q,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "\n",
    "df_vqa = pd.DataFrame(results)\n",
    "df_vqa.to_csv(\"vqa_results.csv\", index=False)\n",
    "df_vqa.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492715d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan ringkasan jawaban\n",
    "summary = df_vqa.groupby([\"experiment\", \"question\"])[\"answer\"].apply(lambda x: x.value_counts().index[0]).reset_index()\n",
    "\n",
    "print(\"Ringkasan jawaban per eksperimen & pertanyaan:\")\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed886fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pertanyaan tertentu\n",
    "q_filter = \"Apakah terdapat teks onomatope di dalam gambar?\"\n",
    "subset = df_vqa[df_vqa[\"question\"] == q_filter]\n",
    "\n",
    "count_answers = subset.groupby([\"experiment\", \"answer\"]).size().unstack(fill_value=0)\n",
    "\n",
    "count_answers.plot(kind=\"bar\", stacked=True, figsize=(10,6))\n",
    "plt.title(\"Distribusi Jawaban VQA per Eksperimen\")\n",
    "plt.ylabel(\"Jumlah Gambar\")\n",
    "plt.xlabel(\"Eksperimen\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Jawaban\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil contoh 1 gambar per eksperimen dengan jawaban VQA\n",
    "sample = df_vqa.groupby(\"experiment\").first().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(sample), figsize=(15,5))\n",
    "for i, row in enumerate(sample.itertuples()):\n",
    "    img_path = os.path.join(base_dir, row.experiment, row.image)\n",
    "    img = Image.open(img_path)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{row.experiment}\\n{row.answer[:15]}...\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
